{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Linear model: $y_i=\\beta_0+\\beta_1 x_{i1}+\\dots+\\beta_p x_{ip}+\\epsilon$ for $i=1,\\dots,n$\n",
    "\n",
    "Matrix notation: $y=X\\beta+\\epsilon$\n",
    "\n",
    "- $y$ - response variable\n",
    "- $x_1, \\dots, x_p$ - set of $p$ regressors\n",
    "- $\\epsilon$ - noise\n",
    "\n",
    "Approximation: $\\hat{y}=X\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "Goal: Find values for $\\beta$ that \"best\" fit the data\n",
    "\n",
    "Question: What's the definition of \"best\"?\n",
    "\n",
    "Error in predictions is the difference between the actual value and the predicted value: $y-\\hat{y}$\n",
    "![Image](https://i1.wp.com/statisticsbyjim.com/wp-content/uploads/2017/04/residuals.png?resize=300%2C186&ssl=1)\n",
    "\n",
    "Squaring the difference accounts for overprediction and underprediction: $(y-\\hat{y})^2$\n",
    "![image](https://miro.medium.com/max/628/1*uBnjPy5o59FfkkMEJL0Nqw.jpeg)\n",
    "\n",
    "The motivation behind OLS is minimizing the sum of squared errors: $\\hat{\\beta} = \\underset{\\beta}{\\operatorname{argmin}} ||y-\\hat{y}||_2^2$\n",
    "\n",
    "OLS is BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of OLS\n",
    "\n",
    "- Linearity - $E[y]=X\\beta$\n",
    "- Strict exogeneity - $E[\\epsilon|X]=0$\n",
    "- No perfect multicollinearity - Regressors can't be linearly dependent, X has full rank, $\\Pr[\\text{rank}(X)=p]=1$\n",
    "- Independent errors\n",
    "- Homoscedasticity - $E[\\epsilon_i^2|X]=\\sigma^2$\n",
    "- No autocorrelation - $E[\\epsilon_i\\epsilon_j|X]=0$ for $i\\neq j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving OLS\n",
    "\n",
    "Goal: Minimize our objective $||y-\\hat{y}||_2^2$\n",
    "\n",
    "Idea: Take the derivative, set equal to 0, and solve for $\\hat{\\beta}$\n",
    "\n",
    "\\begin{align*}\n",
    "||y-\\hat{y}||_2^2 &= (y-\\hat{y})^T (y-\\hat{y}) \\\\\n",
    "&= (y-X\\hat{\\beta})^T (y-X\\hat{\\beta}) \\\\\n",
    "&= y^Ty - \\hat{\\beta}^TX^ty - y^TX\\hat{\\beta} + \\hat{\\beta}^TX^TX\\hat{\\beta} \\\\\n",
    "&= y^Ty - 2\\hat{\\beta}^TX^ty + \\hat{\\beta}^TX^TX\\hat{\\beta} \\\\\n",
    "\\\\\n",
    "\\nabla_\\beta ||y-\\hat{y}||_2^2 &= -2X^Ty + 2X^TX\\hat{\\beta} \\\\\n",
    "\\\\\n",
    "-2X^Ty + 2X^TX\\hat{\\beta} &= 0 \\\\\n",
    "\\Rightarrow X^TX\\hat{\\beta} &= X^Ty \\\\\n",
    "\\Rightarrow \\hat{\\beta} &= (X^TX)^{-1}X^Ty\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset\n",
    "\n",
    "Generate 1000 samples of $X_1 \\sim U(0, 10)$\n",
    "\n",
    "Generate 1000 samples of $X_2 \\sim U(-20, -10)$\n",
    "\n",
    "Generate $y = 2 + 10X_1 - 5X_2 + \\epsilon$ where $\\epsilon \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "x_1 = np.random.uniform(0,10,1000)\n",
    "x_2 = np.random.uniform(-20,-10,1000)\n",
    "y = 2 + 10*x_1 - 5*x_2 + np.random.normal(0, 1, 1000)\n",
    "print(x_1.shape)\n",
    "print(x_2.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for $\\beta$\n",
    "\n",
    "1. Use the closed form solution derived above (HINT: you need to add a column of ones)\n",
    "\n",
    "2. Use sklearn's linear regression\n",
    "\n",
    "Check that estimated coefficients from methods 1 and 2 are the same and match the data generating process.\n",
    "\n",
    "Why do we need to add a column of ones for method 1? What happens if we don't add any noise ($\\epsilon$) to our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n",
      "\n",
      "With ones column:\n",
      "Before invert [ 3.22237853e+10  1.65691534e+11 -4.96822341e+11]\n",
      "After invert [ 1.81762706  9.99255568 -5.01306829]\n",
      "sk linear regression   [ 9.99255568 -5.01306829]\n",
      "\n",
      "Without ones column:\n",
      "After invert (without noise) [10.03025659 -5.11836983]\n",
      "sk linear regression   [ 9.99255568 -5.01306829]\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "#all_data = np.hstack((my_data, new_col))\n",
    "ones_col =  np.ones(1000);\n",
    "print(x_1.shape)\n",
    "print(ones_col.shape)\n",
    "# add all x1, x2 and a ones column into matrix\n",
    "X = np.column_stack((x_1, x_2))\n",
    "# print('stacked shape without ones', X.shape)\n",
    "# print('x', X)\n",
    "X_matrix = np.column_stack((ones_col, x_1, x_2))\n",
    "# print('stacked shape with ones', X_matrix.shape)\n",
    "# print('X_matrix shape', X_matrix.shape )\n",
    "# print(X_matrix)\n",
    "# print('X_matrix.T shape', X_matrix.T.shape )\n",
    "# print(X_matrix.T)\n",
    "# print('X_matrix.T.dot(X_matrix)', X_matrix.T.dot(X_matrix))\n",
    "# print('(X_matrix.T.dot(X_matrix)).dot(X_matrix.T)', (X_matrix.T.dot(X_matrix)).dot(X_matrix.T))\n",
    "\n",
    "# from solution\n",
    "print('\\nWith ones column:')\n",
    "comput =(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y)\n",
    "print('Before invert', (X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y))\n",
    "print('After invert', np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y))\n",
    "print('sk linear regression  ', sk.linear_model.LinearRegression().fit(X, y).coef_)\n",
    "\n",
    "print('\\nWithout ones column:')\n",
    "print('After invert (without noise)', np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y))\n",
    "print('sk linear regression  ', sk.linear_model.LinearRegression().fit(X, y).coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ones columns is needed to create an offset for the coefficient calculation. \n"
     ]
    }
   ],
   "source": [
    "# Insert answers here\n",
    "print('The ones columns is needed to create an offset for the coefficient calculation. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization helps prevent overfitting and increases the generalizability of your model. By adding regularization, we are reducing variance at the expense of bias in our model. The two most common forms of regularization for linear regression are ridge regression and LASSO. These two regularization methods add a L2 or L1 penalty term to the objective, respectively.\n",
    "\n",
    "Ridge regression: $||y-\\hat{y}||_2^2 + \\lambda||\\beta||_2^2$\n",
    "\n",
    "LASSO: $||y-\\hat{y}||_2^2 + \\lambda||\\beta||_1$\n",
    "\n",
    "Ridge regression is able to shrink all coefficients towards 0 while LASSO can set some coefficients towards 0. Because of this, LASSO is also able to perform feature selection. The last section of this blog post explains this concept visually. https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "\n",
    "In order to effectively use regularization, all the regressors ($X$) must be standardized so coefficients can be compared with each other and penalized accordingly. For both of these methods, the regularization parameters needs to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does standardization change the coefficients but not the $R^2$ score of OLS?\n",
    "\n",
    "Why does standarization change the coefficients and the $R^2$ score for LASSO?\n",
    "\n",
    "Derive the closed form solution to ridge regression (HINT: you should get $\\hat{\\beta}=(X^TX+\\lambda I)^{-1}X^Ty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS shrinks all coefficients and keeps the same predicted values,\n",
      "  making r2 remain constant.\n",
      "LASSO shrinks some coefficients, but not all,\n",
      "  changing the predicted values, and therefore, the r2 is changed.\n"
     ]
    }
   ],
   "source": [
    "# Insert answers here\n",
    "print('OLS shrinks all coefficients and keeps the same predicted values,')\n",
    "print('  making r2 remain constant.')\n",
    "print('LASSO shrinks some coefficients, but not all,')\n",
    "print('  changing the predicted values, and therefore, the r2 is changed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge [ 1.71763012  9.99416147 -5.01902585]\n",
      "[ 0.          9.99249635 -5.0130399 ]\n"
     ]
    }
   ],
   "source": [
    "# Insert derivation here\n",
    "\n",
    "# Manual attempt based on formula\n",
    "ridge = np.linalg.inv(X_matrix.T.dot(X_matrix) + 0.5).dot(X_matrix.T).dot(y)\n",
    "print('ridge', ridge)\n",
    "\n",
    "# With sklearn's Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=0.05)\n",
    "rr.fit(X_matrix, y)\n",
    "print(rr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset from the previous section, add another variable $X_3 = .5 X_1 + .5 X_2 + \\epsilon$ where $\\epsilon \\sim N(0, 0.1)$\n",
    "\n",
    "Generate $y = 2 + 10X_1 - 5X_2 + 7X_3 + \\epsilon$ where $\\epsilon \\sim N(0, 1)$\n",
    "\n",
    "Do not standardize your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# Insert code here\n",
    "# ones_col = np.ones(1000);\n",
    "# x_1 = np.random.uniform(0,10,1000)\n",
    "# x_2 = np.random.uniform(-20,-10,1000)\n",
    "x_3 = 0.5*x_1 + 0.5*x_2 + np.random.normal(0,0.1,1000)\n",
    "# print(x_3)\n",
    "y_col = 2 + 10*x_1 - 5*x_2 + 7*x_3 + np.random.normal(0, 1, 1000)\n",
    "# print(y)\n",
    "X_matrix = np.column_stack((x_1, x_2, x_3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OLS to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Use ridge regression with the default hyperparameters to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Use LASSO with the default hyperparameters to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Explain the behavior of these three methods.\n",
    "\n",
    "What would happen if $X_3 = .5X_1 + .5X_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.23552872 -4.7690983   6.51800259]\n",
      "[10.40485227 -4.59740971  6.17625113]\n",
      "[13.36937373 -1.39195893  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "print(sk.linear_model.LinearRegression().fit(X_matrix, y_col).coef_)\n",
    "print(sk.linear_model.Ridge().fit(X_matrix, y_col).coef_)\n",
    "print(sk.linear_model.Lasso().fit(X_matrix, y_col).coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.08988892e+10  8.08988892e+10 -1.61797778e+11]\n",
      "[11.49553931 -3.48289006  4.00632463]\n",
      "[13.38095199 -1.36754194  0.        ]\n",
      "OLS is badly affected by the uniformity of X3 to X1 and X2. It breaks the OLS rule \"No perfect multicollinearity\"\n",
      "Rindge is less aaffected\n",
      "LASSO is barely affected\n"
     ]
    }
   ],
   "source": [
    "# Insert answers here\n",
    "# x_1 = np.random.uniform(0,10,1000)\n",
    "# x_2 = np.random.uniform(-20,-10,1000)\n",
    "x_3 = 0.5*x_1 + 0.5*x_2\n",
    "# print(x_3)\n",
    "y_col = 2 + 10*x_1 - 5*x_2 + 7*x_3 + np.random.normal(0, 1, 1000)\n",
    "# print(y)\n",
    "X_matrix = np.column_stack((x_1, x_2, x_3))\n",
    "print(sk.linear_model.LinearRegression().fit(X_matrix, y_col).coef_)\n",
    "print(sk.linear_model.Ridge().fit(X_matrix, y_col).coef_)\n",
    "print(sk.linear_model.Lasso().fit(X_matrix, y_col).coef_)\n",
    "\n",
    "print('OLS is badly affected by the uniformity of X3 to X1 and X2. It breaks the OLS rule \"No perfect multicollinearity\"')\n",
    "print('Rindge is less aaffected')\n",
    "print('LASSO is barely affected')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sources\n",
    "- https://statisticsbyjim.com/glossary/ordinary-least-squares/\n",
    "- https://medium.com/@saahil1292/machine-learning-102-linear-regression-ordinary-least-squares-ols-correlation-and-analysis-of-7d53751ea9f4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
