{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Linear model: $y_i=\\beta_0+\\beta_1 x_{i1}+\\dots+\\beta_p x_{ip}+\\epsilon$ for $i=1,\\dots,n$\n",
    "\n",
    "Matrix notation: $y=X\\beta+\\epsilon$\n",
    "\n",
    "- $y$ - response variable\n",
    "- $x_1, \\dots, x_p$ - set of $p$ regressors\n",
    "- $\\epsilon$ - noise\n",
    "\n",
    "Approximation: $\\hat{y}=X\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "Goal: Find values for $\\beta$ that \"best\" fit the data\n",
    "\n",
    "Question: What's the definition of \"best\"?\n",
    "\n",
    "Error in predictions is the difference between the actual value and the predicted value: $y-\\hat{y}$\n",
    "![Image](https://i1.wp.com/statisticsbyjim.com/wp-content/uploads/2017/04/residuals.png?resize=300%2C186&ssl=1)\n",
    "\n",
    "Squaring the difference accounts for overprediction and underprediction: $(y-\\hat{y})^2$\n",
    "![image](https://miro.medium.com/max/628/1*uBnjPy5o59FfkkMEJL0Nqw.jpeg)\n",
    "\n",
    "The motivation behind OLS is minimizing the sum of squared errors: $\\hat{\\beta} = \\underset{\\beta}{\\operatorname{argmin}} ||y-\\hat{y}||_2^2$\n",
    "\n",
    "OLS is BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of OLS\n",
    "\n",
    "- Linearity - $E[y]=X\\beta$\n",
    "- Strict exogeneity - $E[\\epsilon|X]=0$\n",
    "- No perfect multicollinearity - Regressors can't be linearly dependent, X has full rank, $\\Pr[\\text{rank}(X)=p]=1$\n",
    "- Independent errors\n",
    "- Homoscedasticity - $E[\\epsilon_i^2|X]=\\sigma^2$\n",
    "- No autocorrelation - $E[\\epsilon_i\\epsilon_j|X]=0$ for $i\\neq j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving OLS\n",
    "\n",
    "Goal: Minimize our objective $||y-\\hat{y}||_2^2$\n",
    "\n",
    "Idea: Take the derivative, set equal to 0, and solve for $\\hat{\\beta}$\n",
    "\n",
    "\\begin{align*}\n",
    "||y-\\hat{y}||_2^2 &= (y-\\hat{y})^T (y-\\hat{y}) \\\\\n",
    "&= (y-X\\hat{\\beta})^T (y-X\\hat{\\beta}) \\\\\n",
    "&= y^Ty - \\hat{\\beta}^TX^ty - y^TX\\hat{\\beta} + \\hat{\\beta}^TX^TX\\hat{\\beta} \\\\\n",
    "&= y^Ty - 2\\hat{\\beta}^TX^ty + \\hat{\\beta}^TX^TX\\hat{\\beta} \\\\\n",
    "\\\\\n",
    "\\nabla_\\beta ||y-\\hat{y}||_2^2 &= -2X^Ty + 2X^TX\\hat{\\beta} \\\\\n",
    "\\\\\n",
    "-2X^Ty + 2X^TX\\hat{\\beta} &= 0 \\\\\n",
    "\\Rightarrow X^TX\\hat{\\beta} &= X^Ty \\\\\n",
    "\\Rightarrow \\hat{\\beta} &= (X^TX)^{-1}X^Ty\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset\n",
    "\n",
    "Generate 1000 samples of $X_1 \\sim U(0, 10)$\n",
    "\n",
    "Generate 1000 samples of $X_2 \\sim U(-20, -10)$\n",
    "\n",
    "Generate $y = 2 + 10X_1 - 5X_2 + \\epsilon$ where $\\epsilon \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "X = np.vstack((np.random.uniform(0, 10, n_samples), np.random.uniform(-20, -10, n_samples))).T\n",
    "print(X.shape)\n",
    "# print(X)\n",
    "y = 2 + 10*X[:, 0]-5*X[:, 1] + np.random.normal(0, 1, n_samples)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for $\\beta$\n",
    "\n",
    "1. Use the closed form solution derived above (HINT: you need to add a column of ones)\n",
    "\n",
    "2. Use sklearn's linear regression\n",
    "\n",
    "Check that estimated coefficients from methods 1 and 2 are the same and match the data generating process.\n",
    "\n",
    "Why do we need to add a column of ones for method 1? What happens if we don't add any noise ($\\epsilon$) to our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[  5.48813504 -14.07119729]\n",
      " [  7.15189366 -19.89936304]\n",
      " [  6.02763376 -15.24173804]\n",
      " ...\n",
      " [  9.38412022 -15.19892193]\n",
      " [  2.28646551 -13.56135963]\n",
      " [  6.77141144 -14.98226869]]\n",
      "X_matrix shape (1000, 3)\n",
      "[[  1.           5.48813504 -14.07119729]\n",
      " [  1.           7.15189366 -19.89936304]\n",
      " [  1.           6.02763376 -15.24173804]\n",
      " ...\n",
      " [  1.           9.38412022 -15.19892193]\n",
      " [  1.           2.28646551 -13.56135963]\n",
      " [  1.           6.77141144 -14.98226869]]\n",
      "\n",
      "with ones column:\n",
      "[ 3.22237853e+10  1.65691534e+11 -4.96822341e+11]\n",
      "[ 1.81762706  9.99255568 -5.01306829]\n",
      "[ 9.99255568 -5.01306829]\n",
      "without ones column:\n",
      "[10.03025659 -5.11836983]\n",
      "[ 9.99255568 -5.01306829]\n"
     ]
    }
   ],
   "source": [
    "X_matrix = np.vstack((np.ones((X.shape[0])), X.T)).T\n",
    "print('X', X)\n",
    "print('X_matrix shape', X_matrix.shape )\n",
    "print(X_matrix)\n",
    "\n",
    "print('\\nwith ones column:')\n",
    "comput =(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y)\n",
    "print((X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y))\n",
    "print(np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y))\n",
    "print(sk.linear_model.LinearRegression().fit(X, y).coef_)\n",
    "\n",
    "print('without ones column:')\n",
    "print(np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y))\n",
    "print(sk.linear_model.LinearRegression().fit(X, y).coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column of ones is needed in order to properly estimate $\\beta_0$. When we go from $y=\\beta_0+\\beta_1 X_1+\\beta_2 X_2$ to $y=\\beta X$, we are adding in the ones in between as $y=\\beta_0 1 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
    "\n",
    "If we don't add noise to our data, our estimated coefficients will be the same as the coefficients we used to generate our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization helps prevent overfitting and increases the generalizability of your model. By adding regularization, we are reducing variance at the expense of bias in our model. The two most common forms of regularization for linear regression are ridge regression and LASSO. These two regularization methods add a L2 or L1 penalty term to the objective, respectively.\n",
    "\n",
    "Ridge regression: $||y-\\hat{y}||_2^2 + \\lambda||\\beta||_2^2$\n",
    "\n",
    "LASSO: $||y-\\hat{y}||_2^2 + \\lambda||\\beta||_1$\n",
    "\n",
    "Ridge regression is able to shrink all coefficients towards 0 while LASSO can set some coefficients towards 0. Because of this, LASSO is also able to perform feature selection. The last section of this blog post explains this concept visually. https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "\n",
    "In order to effectively use regularization, all the regressors ($X$) must be standardized so coefficients can be compared with each other and penalized accordingly. For both of these methods, the regularization parameters needs to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does standardization change the coefficients but not the $R^2$ score of OLS?\n",
    "\n",
    "Why does standarization change the coefficients and the $R^2$ score for LASSO?\n",
    "\n",
    "Derive the closed form solution to ridge regression (HINT: you should get $\\hat{\\beta}=(X^TX+\\lambda I)^{-1}X^Ty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OLS, the coefficients will be the inverse transformation of the standardization. The $R^2$ score (and any measure of fit) stays the same because the predicted values stay the same. For LASSO, the coefficents change because the coefficents no longer depend on the scale of the data which can lead to different coefficients being set to 0, changing the predicted value and the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert derivation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can augment $X$ and $y$ and solve as previously shown. More details are given in the 2nd answer of https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset from the previous section, add another variable $X_3 = .5 X_1 + .5 X_2 + \\epsilon$ where $\\epsilon \\sim N(0, 0.1)$\n",
    "\n",
    "Generate $y = 2 + 10X_1 - 5X_2 + 7X_3 + \\epsilon$ where $\\epsilon \\sim N(0, 1)$\n",
    "\n",
    "Do not standardize your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col = np.vstack((X.T, .5*X[:, 0] + .5*X[:, 1] + np.random.normal(0, .1, X.shape[0]))).T\n",
    "y_col = y+7*X_col[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OLS to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Use ridge regression with the default hyperparameters to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Use LASSO with the default hyperparameters to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Explain the behavior of these three methods.\n",
    "\n",
    "What would happen if $X_3 = .5X_1 + .5X_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.83554888 -5.17063487  7.31459695]\n",
      "[10.05867043 -4.94495873  6.86513126]\n",
      "[13.36695253 -1.39451947  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(sk.linear_model.LinearRegression().fit(X_col, y_col).coef_)\n",
    "print(sk.linear_model.Ridge().fit(X_col, y_col).coef_)\n",
    "print(sk.linear_model.Lasso().fit(X_col, y_col).coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS and ridge regression estimates are close to the true values.  Ridge regression splits the contribution of $X_3$ between $X_1$ and $X_2$. If we changed $X_3$ as above, then OLS would not longer be valid as a result of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sources\n",
    "- https://statisticsbyjim.com/glossary/ordinary-least-squares/\n",
    "- https://medium.com/@saahil1292/machine-learning-102-linear-regression-ordinary-least-squares-ols-correlation-and-analysis-of-7d53751ea9f4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
